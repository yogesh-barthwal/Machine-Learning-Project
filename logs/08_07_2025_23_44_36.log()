[2025-08-07 23:44:37,048] 27 root - INFO - Entered the data ingestion component
[2025-08-07 23:44:37,053] 30 root - INFO - Read data as a dataframe
[2025-08-07 23:44:37,073] 36 root - INFO - Train and test data split initiated
[2025-08-07 23:44:37,096] 41 root - INFO - Data ingestion completed
[2025-08-07 23:44:37,101] 83 root - INFO - Train and Test data read
[2025-08-07 23:44:37,101] 84 root - INFO - Obtaining preprocessing object
[2025-08-07 23:44:37,101] 52 root - INFO - Log Transformation performed for skewed features
[2025-08-07 23:44:37,101] 70 root - INFO - 'fixed acidity', 'free sulfur dioxide' dropped owing to multi collinearity
[2025-08-07 23:44:37,102] 96 root - INFO - Applying preprocessing on train and test dataframe
[2025-08-07 23:44:37,122] 104 root - INFO - Saving preprocessing object
[2025-08-07 23:44:37,127] 19 root - INFO - Object successfuly saved at artifacts\preprocessor.pkl
[2025-08-07 23:44:37,128] 28 root - INFO - Splitting train and test input data
[2025-08-07 23:44:37,129] 81 root - INFO - Training Model LogisticRegression(max_iter=1500, random_state=42)
[2025-08-07 23:44:37,129] 84 root - INFO - Hyperparameter tuning for Logistic Regression
[2025-08-07 23:44:37,980] 88 root - INFO - Best params forLogistic Regression:{'C': 10, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}
[2025-08-07 23:44:37,983] 102 root - INFO - Logistic Regression--> Accuracy: 0.578125, F1-macro: 0.28876100751100753, F1-weighted: 0.5545299752331003
[2025-08-07 23:44:37,984] 81 root - INFO - Training Model DecisionTreeClassifier(max_depth=4, random_state=42)
[2025-08-07 23:44:37,984] 84 root - INFO - Hyperparameter tuning for Decision Tree
[2025-08-07 23:44:38,746] 88 root - INFO - Best params forDecision Tree:{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2}
[2025-08-07 23:44:38,750] 102 root - INFO - Decision Tree--> Accuracy: 0.571875, F1-macro: 0.28791941701309204, F1-weighted: 0.5590444435502573
[2025-08-07 23:44:38,750] 81 root - INFO - Training Model RandomForestClassifier(random_state=42)
[2025-08-07 23:44:38,750] 84 root - INFO - Hyperparameter tuning for Random Forest
[2025-08-07 23:45:12,102] 88 root - INFO - Best params forRandom Forest:{'bootstrap': True, 'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}
[2025-08-07 23:45:12,118] 102 root - INFO - Random Forest--> Accuracy: 0.678125, F1-macro: 0.33676998251023244, F1-weighted: 0.6601034510737275
[2025-08-07 23:45:12,119] 81 root - INFO - Training Model KNeighborsClassifier(n_neighbors=3)
[2025-08-07 23:45:12,119] 84 root - INFO - Hyperparameter tuning for K Nearest Neighbours
[2025-08-07 23:45:12,462] 88 root - INFO - Best params forK Nearest Neighbours:{'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'distance'}
[2025-08-07 23:45:12,477] 102 root - INFO - K Nearest Neighbours--> Accuracy: 0.64375, F1-macro: 0.32764252967205, F1-weighted: 0.6323573605054232
[2025-08-07 23:45:12,478] 81 root - INFO - Training Model GaussianNB()
[2025-08-07 23:45:12,478] 84 root - INFO - Hyperparameter tuning for Naive Bayes
[2025-08-07 23:45:12,509] 88 root - INFO - Best params forNaive Bayes:{'var_smoothing': 1e-09}
[2025-08-07 23:45:12,513] 102 root - INFO - Naive Bayes--> Accuracy: 0.55, F1-macro: 0.3249260630275966, F1-weighted: 0.5399222927316528
[2025-08-07 23:45:12,513] 81 root - INFO - Training Model SVC(random_state=42)
[2025-08-07 23:45:12,513] 84 root - INFO - Hyperparameter tuning for Support Vector Machine
[2025-08-07 23:45:14,742] 88 root - INFO - Best params forSupport Vector Machine:{'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}
[2025-08-07 23:45:14,768] 102 root - INFO - Support Vector Machine--> Accuracy: 0.61875, F1-macro: 0.3212040853169886, F1-weighted: 0.6060640529642546
[2025-08-07 23:45:14,769] 54 root - INFO - Best model identified-->RandomForestClassifier(min_samples_split=5, n_estimators=200, random_state=42). Saving to disk
[2025-08-07 23:45:14,790] 19 root - INFO - Object successfuly saved at artifacts\model.pkl
[2025-08-07 23:45:14,807] 28 root - INFO - Splitting train and test input data
[2025-08-07 23:45:14,808] 81 root - INFO - Training Model LogisticRegression(max_iter=1500, random_state=42)
[2025-08-07 23:45:14,808] 84 root - INFO - Hyperparameter tuning for Logistic Regression
[2025-08-07 23:45:15,728] 88 root - INFO - Best params forLogistic Regression:{'C': 10, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}
[2025-08-07 23:45:15,731] 102 root - INFO - Logistic Regression--> Accuracy: 0.578125, F1-macro: 0.28876100751100753, F1-weighted: 0.5545299752331003
[2025-08-07 23:45:15,732] 81 root - INFO - Training Model DecisionTreeClassifier(max_depth=4, random_state=42)
[2025-08-07 23:45:15,732] 84 root - INFO - Hyperparameter tuning for Decision Tree
[2025-08-07 23:45:16,297] 88 root - INFO - Best params forDecision Tree:{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2}
[2025-08-07 23:45:16,301] 102 root - INFO - Decision Tree--> Accuracy: 0.571875, F1-macro: 0.28791941701309204, F1-weighted: 0.5590444435502573
[2025-08-07 23:45:16,302] 81 root - INFO - Training Model RandomForestClassifier(random_state=42)
[2025-08-07 23:45:16,302] 84 root - INFO - Hyperparameter tuning for Random Forest
[2025-08-07 23:45:49,589] 88 root - INFO - Best params forRandom Forest:{'bootstrap': True, 'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}
[2025-08-07 23:45:49,609] 102 root - INFO - Random Forest--> Accuracy: 0.678125, F1-macro: 0.33676998251023244, F1-weighted: 0.6601034510737275
[2025-08-07 23:45:49,609] 81 root - INFO - Training Model KNeighborsClassifier(n_neighbors=3)
[2025-08-07 23:45:49,609] 84 root - INFO - Hyperparameter tuning for K Nearest Neighbours
[2025-08-07 23:45:50,029] 88 root - INFO - Best params forK Nearest Neighbours:{'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'distance'}
[2025-08-07 23:45:50,042] 102 root - INFO - K Nearest Neighbours--> Accuracy: 0.64375, F1-macro: 0.32764252967205, F1-weighted: 0.6323573605054232
[2025-08-07 23:45:50,042] 81 root - INFO - Training Model GaussianNB()
[2025-08-07 23:45:50,042] 84 root - INFO - Hyperparameter tuning for Naive Bayes
[2025-08-07 23:45:50,071] 88 root - INFO - Best params forNaive Bayes:{'var_smoothing': 1e-09}
[2025-08-07 23:45:50,074] 102 root - INFO - Naive Bayes--> Accuracy: 0.55, F1-macro: 0.3249260630275966, F1-weighted: 0.5399222927316528
[2025-08-07 23:45:50,074] 81 root - INFO - Training Model SVC(random_state=42)
[2025-08-07 23:45:50,075] 84 root - INFO - Hyperparameter tuning for Support Vector Machine
[2025-08-07 23:45:52,428] 88 root - INFO - Best params forSupport Vector Machine:{'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}
[2025-08-07 23:45:52,463] 102 root - INFO - Support Vector Machine--> Accuracy: 0.61875, F1-macro: 0.3212040853169886, F1-weighted: 0.6060640529642546
[2025-08-07 23:45:52,465] 54 root - INFO - Best model identified-->RandomForestClassifier(min_samples_split=5, n_estimators=200, random_state=42). Saving to disk
[2025-08-07 23:45:52,484] 19 root - INFO - Object successfuly saved at artifacts\model.pkl
[2025-08-07 23:45:52,503] 59 root - INFO - Model Training F1 Score: 0.6601034510737275
